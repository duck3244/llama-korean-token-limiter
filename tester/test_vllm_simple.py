#!/usr/bin/env python3
"""
vLLM ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import subprocess
import time

def test_vllm_import():
    """vLLM íŒ¨í‚¤ì§€ import í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª vLLM import í…ŒìŠ¤íŠ¸...")
    try:
        import vllm
        print(f"âœ… vLLM ë²„ì „: {vllm.__version__}")
        return True
    except ImportError as e:
        print(f"âŒ vLLM import ì‹¤íŒ¨: {e}")
        return False

def test_cuda():
    """CUDA í™˜ê²½ í…ŒìŠ¤íŠ¸"""
    print("ğŸ® CUDA í…ŒìŠ¤íŠ¸...")
    try:
        import torch
        print(f"PyTorch ë²„ì „: {torch.__version__}")
        print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA ë²„ì „: {torch.version.cuda}")
            print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            print(f"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
            
            # GPU ë©”ëª¨ë¦¬ í™•ì¸
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            cached = torch.cuda.memory_reserved(0) / 1024**3
            print(f"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.1f}GB")
            print(f"ìºì‹œëœ ë©”ëª¨ë¦¬: {cached:.1f}GB")
            
        return torch.cuda.is_available()
    except Exception as e:
        print(f"âŒ CUDA í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_small_model():
    """ì‘ì€ ëª¨ë¸ë¡œ vLLM í…ŒìŠ¤íŠ¸"""
    print("ğŸ¤– ì‘ì€ ëª¨ë¸ë¡œ vLLM í…ŒìŠ¤íŠ¸...")
    
    # RTX 4060 8GBì— ì í•©í•œ ì‘ì€ ëª¨ë¸ë“¤
    small_models = [
        "microsoft/DialoGPT-medium",  # ~350MB
        "gpt2",  # ~500MB
        "distilgpt2",  # ~320MB
    ]
    
    for model_name in small_models:
        print(f"\nğŸ” {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        try:
            from vllm import LLM
            
            # ë§¤ìš° ë³´ìˆ˜ì ì¸ ì„¤ì •
            llm = LLM(
                model=model_name,
                gpu_memory_utilization=0.5,  # 50%ë§Œ ì‚¬ìš©
                max_model_len=512,  # ì§§ì€ ì»¨í…ìŠ¤íŠ¸
                dtype="half",
                enforce_eager=True,
                trust_remote_code=True,
                tensor_parallel_size=1
            )
            
            # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
            prompts = ["Hello, how are you?"]
            outputs = llm.generate(prompts, max_tokens=10)
            
            for output in outputs:
                print(f"âœ… ìƒì„± ì„±ê³µ: {output.outputs[0].text}")
            
            print(f"âœ… {model_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            return True
            
        except Exception as e:
            print(f"âŒ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            continue
    
    return False

def test_vllm_server():
    """vLLM ì„œë²„ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ vLLM ì„œë²„ ëª¨ë“œ í…ŒìŠ¤íŠ¸...")
    
    # ê°€ì¥ ì‘ì€ ëª¨ë¸ë¡œ ì„œë²„ ì‹œì‘
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", "distilgpt2",
        "--port", "8001",  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
        "--host", "127.0.0.1",
        "--gpu-memory-utilization", "0.4",
        "--max-model-len", "256",
        "--dtype", "half",
        "--enforce-eager",
        "--trust-remote-code"
    ]
    
    print(f"ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    try:
        # ì„œë²„ ì‹œì‘
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # 10ì´ˆê°„ ëŒ€ê¸°
        for i in range(10):
            if process.poll() is not None:
                stdout, stderr = process.communicate()
                print(f"âŒ ì„œë²„ê°€ {i}ì´ˆ í›„ ì¢…ë£Œë¨")
                print(f"stdout: {stdout[-500:]}")
                print(f"stderr: {stderr[-500:]}")
                return False
            
            print(f"â³ ì„œë²„ ì‹œì‘ ëŒ€ê¸° ì¤‘... ({i+1}/10)")
            time.sleep(1)
        
        # ì„œë²„ ì¢…ë£Œ
        process.terminate()
        process.wait(timeout=5)
        
        print("âœ… ì„œë²„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ì •ìƒ ì‹œì‘ë¨)")
        return True
        
    except Exception as e:
        print(f"âŒ ì„œë²„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def check_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸"""
    print("ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸...")
    
    try:
        import psutil
        
        # CPU ì •ë³´
        print(f"CPU ì½”ì–´: {psutil.cpu_count()}")
        print(f"CPU ì‚¬ìš©ë¥ : {psutil.cpu_percent()}%")
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        print(f"ì´ ë©”ëª¨ë¦¬: {memory.total / 1024**3:.1f}GB")
        print(f"ì‚¬ìš© ë©”ëª¨ë¦¬: {memory.used / 1024**3:.1f}GB")
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent}%")
        
        # ë””ìŠ¤í¬ ì •ë³´
        disk = psutil.disk_usage('.')
        print(f"ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„: {disk.free / 1024**3:.1f}GB")
        
        return True
        
    except ImportError:
        print("psutil íŒ¨í‚¤ì§€ê°€ ì—†ì–´ì„œ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return True
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("ğŸ§ª vLLM ì§„ë‹¨ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸
    tests = [
        ("vLLM íŒ¨í‚¤ì§€", test_vllm_import),
        ("CUDA í™˜ê²½", test_cuda),
        ("ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤", check_system_resources),
        ("ì‘ì€ ëª¨ë¸", test_small_model),
        ("ì„œë²„ ëª¨ë“œ", test_vllm_server),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{'=' * 20}")
        print(f"ğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸")
        print(f"{'=' * 20}")
        
        try:
            result = test_func()
            results[test_name] = result
            status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
            print(f"\nğŸ {test_name}: {status}")
        except Exception as e:
            print(f"\nğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            results[test_name] = False
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    
    for test_name, result in results.items():
        status = "âœ…" if result else "âŒ"
        print(f"{status} {test_name}")
    
    # ê¶Œì¥ì‚¬í•­
    print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
    
    if not results.get("CUDA í™˜ê²½", False):
        print("â— CUDA í™˜ê²½ ë¬¸ì œ. PyTorch CUDA ì„¤ì¹˜ í™•ì¸ í•„ìš”")
    
    if not results.get("ì‘ì€ ëª¨ë¸", False):
        print("â— vLLM ê¸°ë³¸ ë™ì‘ ë¬¸ì œ. íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜ ê¶Œì¥")
        print("   pip uninstall vllm")
        print("   pip install vllm==0.2.7")
    
    if not results.get("ì„œë²„ ëª¨ë“œ", False):
        print("â— vLLM ì„œë²„ ëª¨ë“œ ë¬¸ì œ")
        print("   1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±")
        print("   2. í¬íŠ¸ ì¶©ëŒ í™•ì¸")
        print("   3. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© ê¶Œì¥")
    
    # CPU ì „ìš© ëª¨ë“œ ì œì•ˆ
    if not results.get("CUDA í™˜ê²½", False) or not results.get("ì‘ì€ ëª¨ë¸", False):
        print("\nğŸ’¡ ëŒ€ì•ˆ: CPU ì „ìš© Token Limiter ì‹¤í–‰")
        print("   python main.py  # vLLM ì—†ì´ ì‹¤í–‰")

if __name__ == "__main__":
    main()