#!/bin/bash
# í•œêµ­ì–´ Llama ëª¨ë¸ vLLM ì„œë²„ ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸

set -e

echo "ðŸ‡°ðŸ‡· í•œêµ­ì–´ Llama ëª¨ë¸ vLLM ì„œë²„ ì‹œìž‘ ì¤‘..."

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
echo "ðŸ” GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸:"
nvidia-smi --query-gpu=memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
echo "ðŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘..."
python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print('âœ… CUDA cache cleared')
else:
    print('âŒ CUDA not available')
"

# Python ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
if [[ "$VIRTUAL_ENV" == "" ]]; then
    echo "âš ï¸ ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    if [ -d "venv" ]; then
        echo "ðŸ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
        source venv/bin/activate
    else
        echo "âŒ ê°€ìƒí™˜ê²½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. setup.shë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
        exit 1
    fi
fi

# HuggingFace í† í° í™•ì¸ (í•„ìš” ì‹œ)
if [ ! -z "$HUGGINGFACE_TOKEN" ]; then
    echo "ðŸ”‘ HuggingFace í† í° ì„¤ì •ë¨"
else
    echo "âš ï¸ HuggingFace í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê³µê°œ ëª¨ë¸ì´ë¯€ë¡œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤."
fi

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸
echo "ðŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸ ì¤‘..."
python -c "
try:
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        'torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1',
        cache_dir='./tokenizer_cache'
    )
    print('âœ… í•œêµ­ì–´ ëª¨ë¸ í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ')
    print(f'   ì–´íœ˜ í¬ê¸°: {len(tokenizer):,}')
except Exception as e:
    print(f'âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}')
    print('ðŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤...')
    exit(1)
"

if [ $? -ne 0 ]; then
    echo "ðŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤)"
    python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
print('í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì¤‘...')
tokenizer = AutoTokenizer.from_pretrained(
    'torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1',
    cache_dir='./tokenizer_cache'
)
print('âœ… í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ ì™„ë£Œ')
"
fi

# vLLM ì„œë²„ ì‹œìž‘ (RTX 4060 8GB ìµœì í™” ì„¤ì •)
echo "ðŸš€ vLLM ì„œë²„ ì‹œìž‘ ì¤‘..."
echo "ðŸ“‹ ì„œë²„ ì„¤ì •:"
echo "   - ëª¨ë¸: torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"
echo "   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : 80%"
echo "   - ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: 2048"
echo "   - ì •ë°€ë„: FP16"
echo "   - í¬íŠ¸: 8000"

exec python -m vllm.entrypoints.openai.api_server \
    --model torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1 \
    --port 8000 \
    --host 0.0.0.0 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 2048 \
    --dtype half \
    --tensor-parallel-size 1 \
    --enforce-eager \
    --trust-remote-code \
    --disable-log-requests \
    --served-model-name korean-llama \
    --chat-template "{% for message in messages %}{{ message.role }}: {{ message.content }}\n{% endfor %}assistant:" \
    --api-key sk-vllm-korean-server-key \
    2>&1 | tee logs/vllm_korean_server.log
