#!/bin/bash
# RTX 4060 ìµœì í™”ëœ vLLM ì„œë²„ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ RTX 4060 ìµœì í™” vLLM ì„œë²„ ì‹œì‘ ì¤‘..."

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
python -c "
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print('âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ')
"

# ì‘ì€ ëª¨ë¸ë¡œ vLLM ì„œë²„ ì‹œì‘ (ê²€ì¦ëœ ì„¤ì •)
exec python -m vllm.entrypoints.openai.api_server \
    --model "distilgpt2" \
    --port 8000 \
    --host 0.0.0.0 \
    --gpu-memory-utilization 0.4 \
    --max-model-len 256 \
    --dtype half \
    --enforce-eager \
    --trust-remote-code \
    --served-model-name korean-llama \
    --disable-log-requests