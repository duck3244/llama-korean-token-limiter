# ğŸ‡°ğŸ‡· Korean Llama Token Limiter

**í•œêµ­ì–´ Llama ëª¨ë¸ìš© í† í° ì‚¬ìš©ëŸ‰ ì œí•œ ì‹œìŠ¤í…œ**

RTX 4060 8GB GPUì— ìµœì í™”ëœ í•œêµ­ì–´ Llama-3.2-Korean ëª¨ë¸ì„ ìœ„í•œ í† í° ì‚¬ìš©ëŸ‰ ê´€ë¦¬ ë° ì†ë„ ì œí•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ íŠ¹í™”**: í•œê¸€ í† í° íŠ¹ì„±ì„ ë°˜ì˜í•œ ì •í™•í•œ í† í° ê³„ì‚°
- âš¡ **ì‹¤ì‹œê°„ ì œí•œ**: ë¶„/ì‹œê°„/ì¼ë³„ í† í° ë° ìš”ì²­ ìˆ˜ ì œí•œ
- ğŸ¯ **RTX 4060 ìµœì í™”**: 8GB VRAM í™˜ê²½ì— ë§ì¶˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìš´ì˜
- ğŸ“Š **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: Streamlit ê¸°ë°˜ í•œêµ­ì–´ ëŒ€ì‹œë³´ë“œ
- ğŸ”„ **ìë™ ë³µêµ¬**: ì¿¨ë‹¤ìš´ ë° ì‚¬ìš©ëŸ‰ ìë™ ì´ˆê¸°í™”
- ğŸ’¾ **ìœ ì—°í•œ ì €ì¥ì†Œ**: Redis ë˜ëŠ” SQLite ì§€ì›
- ğŸ”’ **ì‚¬ìš©ì ê´€ë¦¬**: API í‚¤ ê¸°ë°˜ í•œêµ­ì–´ ì‚¬ìš©ì ì¸ì¦

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚    â”‚  Token Limiter  â”‚    â”‚   vLLM Server   â”‚
â”‚                 â”‚â”€â”€â”€â”€â”‚   (Port 8080)   â”‚â”€â”€â”€â”€â”‚   (Port 8000)   â”‚
â”‚   API ìš”ì²­      â”‚    â”‚  í•œêµ­ì–´ í† í°    â”‚    â”‚ Korean Llama    â”‚
â”‚                 â”‚    â”‚   ì‚¬ìš©ëŸ‰ ì œí•œ   â”‚    â”‚     ëª¨ë¸        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Redis/SQLite  â”‚
                       â”‚   ì‚¬ìš©ëŸ‰ ì €ì¥   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- **OS**: Ubuntu 22.04 (ê¶Œì¥)
- **GPU**: NVIDIA RTX 4060 8GB ì´ìƒ
- **CUDA**: 12.1+
- **Python**: 3.9+
- **RAM**: 16GB ê¶Œì¥
- **ë””ìŠ¤í¬**: 10GB ì´ìƒ

### 2. ìë™ ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-repo/korean-llama-token-limiter.git
cd korean-llama-token-limiter

# ìë™ ì„¤ì¹˜ ì‹¤í–‰
chmod +x setup.sh
./setup.sh

# ì‹œìŠ¤í…œ ì‹œì‘
./scripts/start_korean_system.sh
```

### 3. ìˆ˜ë™ ì„¤ì¹˜

<details>
<summary>ìˆ˜ë™ ì„¤ì¹˜ ë‹¨ê³„ ë³´ê¸°</summary>

```bash
# 1. ì˜ì¡´ì„± ì„¤ì¹˜
sudo apt update
sudo apt install python3.11 python3.11-venv python3-pip build-essential curl git

# 2. Python í™˜ê²½ ì„¤ì •
python3.11 -m venv venv
source venv/bin/activate
pip install --upgrade pip

# 3. PyTorch ì„¤ì¹˜ (CUDA 12.1)
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121

# 4. vLLM ì„¤ì¹˜
pip install vllm==0.2.7

# 5. ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# 6. Redis ì‹œì‘ (Docker)
docker run -d --name korean-redis -p 6379:6379 redis:alpine

# 7. ì‹œìŠ¤í…œ ì‹œì‘
python main_korean.py
```

</details>

## ğŸ”§ ì„¤ì •

### ê¸°ë³¸ ì„¤ì • íŒŒì¼

#### `config/korean_model.yaml`
```yaml
server:
  host: "0.0.0.0"
  port: 8080

llm_server:
  url: "http://localhost:8000"
  model_name: "torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1"

default_limits:
  rpm: 30      # ë¶„ë‹¹ ìš”ì²­ ìˆ˜
  tpm: 5000    # ë¶„ë‹¹ í† í° ìˆ˜
  tph: 300000  # ì‹œê°„ë‹¹ í† í° ìˆ˜
  daily: 500000 # ì¼ì¼ í† í° ìˆ˜
```

#### `config/korean_users.yaml`
```yaml
users:
  ì‚¬ìš©ì1:
    rpm: 20
    tpm: 3000
    daily: 500000
    description: "ì¼ë°˜ ì‚¬ìš©ì 1"

api_keys:
  "sk-user1-korean-key-def": "ì‚¬ìš©ì1"
```

## ğŸ“¡ API ì‚¬ìš©ë²•

### ì±„íŒ… ì™„ì„± ìš”ì²­

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-llama",
    "messages": [
      {
        "role": "system", 
        "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
      },
      {
        "role": "user", 
        "content": "ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì–´ë¡œ ê°„ë‹¨í•œ ì¸ì‚¬ë¥¼ í•´ì£¼ì„¸ìš”."
      }
    ],
    "max_tokens": 150,
    "temperature": 0.7
  }'
```

### ì‚¬ìš©ëŸ‰ ì¡°íšŒ

```bash
# ì‚¬ìš©ì í†µê³„ ì¡°íšŒ
curl http://localhost:8080/stats/ì‚¬ìš©ì1

# ì‹œìŠ¤í…œ ì „ì²´ í†µê³„
curl http://localhost:8080/admin/statistics

# ìƒìœ„ ì‚¬ìš©ì ì¡°íšŒ
curl http://localhost:8080/admin/top-users?limit=10&period=today
```

### í† í° ê³„ì‚°

```bash
curl "http://localhost:8080/token-info?text=ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì–´ í† í° ê³„ì‚° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
```

## ğŸ“Š ëŒ€ì‹œë³´ë“œ

Streamlit ê¸°ë°˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ:

```bash
# ëŒ€ì‹œë³´ë“œ ì‹œì‘
streamlit run dashboard/app.py --server.port 8501

# ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†
open http://localhost:8501
```

### ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥

- ğŸ“ˆ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‚¬ìš©ëŸ‰, ìƒìœ„ ì‚¬ìš©ì, ì‹œìŠ¤í…œ ìƒíƒœ
- ğŸ‘¥ **ì‚¬ìš©ì ê´€ë¦¬**: ê°œë³„ ì‚¬ìš©ì í†µê³„ ë° ì œí•œ ê´€ë¦¬
- ğŸ“Š **í†µê³„ ë¶„ì„**: ê¸°ê°„ë³„ ì‚¬ìš©ëŸ‰ ë¶„ì„ ë° íŠ¸ë Œë“œ
- ğŸ”§ **ì‹œìŠ¤í…œ ê´€ë¦¬**: ì„¤ì • ë¡œë“œ, í† í° ê³„ì‚° í…ŒìŠ¤íŠ¸

## ğŸ³ Docker ì‹¤í–‰

### Docker Compose (ê¶Œì¥)

```bash
# ëª¨ë“  ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# ëª¨ë‹ˆí„°ë§ í¬í•¨ ì‹œì‘
docker-compose --profile monitoring up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f token-limiter
```

### ê°œë³„ Docker ì‹¤í–‰

```bash
# ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t korean-token-limiter .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰ (GPU í¬í•¨)
docker run --gpus all \
  -p 8080:8080 \
  -v $(pwd)/config:/app/config \
  -v $(pwd)/logs:/app/logs \
  korean-token-limiter
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ìë™ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
./scripts/test_korean.sh

# ê°œë³„ API í…ŒìŠ¤íŠ¸
pytest tests/ -v
```

### ìˆ˜ë™ í…ŒìŠ¤íŠ¸

```bash
# ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
curl http://localhost:8080/health

# ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-test-korean-key-stu" \
  -d '{"model": "korean-llama", "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}], "max_tokens": 50}'
```

## ğŸ“ ì‚¬ìš©ì ê´€ë¦¬

### ìƒˆ ì‚¬ìš©ì ì¶”ê°€

1. `config/korean_users.yaml` í¸ì§‘:
```yaml
users:
  ì‹ ê·œì‚¬ìš©ì:
    rpm: 15
    tpm: 2000
    tph: 120000
    daily: 300000
    cooldown_minutes: 5
    description: "ì‹ ê·œ ì‚¬ìš©ì"

api_keys:
  "sk-new-user-key-123": "ì‹ ê·œì‚¬ìš©ì"
```

2. ì„¤ì • ë‹¤ì‹œ ë¡œë“œ:
```bash
curl -X POST http://localhost:8080/admin/reload-config
```

### ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”

```bash
# íŠ¹ì • ì‚¬ìš©ì ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”
curl -X DELETE http://localhost:8080/admin/reset-usage/ì‚¬ìš©ì1
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ë©”ëª¨ë¦¬ ì •ë¦¬
sudo fuser -v /dev/nvidia*
sudo kill -9 <PID>

# ì„¤ì •ì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì¡°ì • (config/korean_model.yaml)
gpu_memory_utilization: 0.7  # 0.8ì—ì„œ 0.7ë¡œ ë‚®ì¶¤
```

#### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# ìºì‹œ ì •ë¦¬
rm -rf ~/.cache/huggingface/

# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    'torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1',
    cache_dir='./tokenizer_cache'
)
print('ë‹¤ìš´ë¡œë“œ ì™„ë£Œ')
"
```

#### 3. Redis ì—°ê²° ì‹¤íŒ¨
```bash
# Redis ìƒíƒœ í™•ì¸
redis-cli ping

# Docker Redis ì¬ì‹œì‘
docker restart korean-redis

# ë˜ëŠ” ë¡œì»¬ Redis ì„¤ì¹˜
sudo apt install redis-server
sudo systemctl start redis
```

#### 4. í¬íŠ¸ ì¶©ëŒ
```bash
# í¬íŠ¸ ì‚¬ìš© í™•ì¸
sudo lsof -i :8080
sudo lsof -i :8000

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo kill -9 <PID>
```

### ë¡œê·¸ í™•ì¸

```bash
# Token Limiter ë¡œê·¸
tail -f logs/token_limiter.log

# vLLM ì„œë²„ ë¡œê·¸
tail -f logs/vllm_korean_server.log

# ì‹œìŠ¤í…œ ì „ì²´ ë¡œê·¸
journalctl -f
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### RTX 4060 8GB ìµœì í™” íŒ

1. **ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •**:
```yaml
# config/korean_model.yaml
vllm_args:
  gpu_memory_utilization: 0.8  # í•„ìš”ì‹œ 0.7ë¡œ ë‚®ì¶¤
  max_model_len: 2048          # ê¸¸ì´ ì¤„ì—¬ì„œ ë©”ëª¨ë¦¬ ì ˆì•½
  dtype: "half"                # FP16 ì‚¬ìš©
```

2. **ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ**:
```yaml
default_limits:
  rpm: 20  # 30ì—ì„œ 20ìœ¼ë¡œ ë‚®ì¶¤
  tpm: 3000  # 5000ì—ì„œ 3000ìœ¼ë¡œ ë‚®ì¶¤
```

3. **ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜**:
```bash
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_LAUNCH_BLOCKING=1
```

## ğŸ”— ìœ ìš©í•œ ë§í¬

- ğŸ“š [vLLM ë¬¸ì„œ](https://docs.vllm.ai/)
- ğŸ¤– [Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬](https://huggingface.co/docs/transformers/)
- ğŸ‡°ğŸ‡· [í•œêµ­ì–´ Llama ëª¨ë¸](https://huggingface.co/torchtorchkimtorch/Llama-3.2-Korean-GGACHI-1B-Instruct-v1)
- ğŸ“Š [Streamlit ë¬¸ì„œ](https://docs.streamlit.io/)

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ ëª¨ë“œ ì„¤ì¹˜
pip install -e .

# ê°œë°œ ë„êµ¬ ì„¤ì¹˜
pip install black flake8 pytest

# ì½”ë“œ í¬ë§·íŒ…
black src/ tests/

# ë¦°íŒ…
flake8 src/ tests/

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v --cov=src/
```
