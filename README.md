# ğŸ‡°ğŸ‡· Korean Llama Token Limiter

í•œêµ­ì–´ íŠ¹í™” LLM í† í° ì‚¬ìš©ëŸ‰ ì œí•œ ì‹œìŠ¤í…œ

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![vLLM](https://img.shields.io/badge/vLLM-0.2.7+-red.svg)](https://github.com/vllm-project/vllm)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ ê°œìš”

Korean Llama Token LimiterëŠ” í•œêµ­ì–´ LLM(Large Language Model) ì„œë¹„ìŠ¤ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì œí•œí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤. RTX 4060 8GB GPU í™˜ê²½ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

- ğŸ”¢ **í•œêµ­ì–´ íŠ¹í™” í† í° ê³„ì‚°**: í•œê¸€ 1ê¸€ì â‰ˆ 1.2í† í°ìœ¼ë¡œ ì •í™•í•œ ê³„ì‚°
- âš¡ **ì‹¤ì‹œê°„ ì†ë„ ì œí•œ**: ë¶„ë‹¹/ì‹œê°„ë‹¹/ì¼ì¼ í† í° ì‚¬ìš©ëŸ‰ ì œí•œ
- ğŸ‘¥ **ë‹¤ì¤‘ ì‚¬ìš©ì ê´€ë¦¬**: API í‚¤ ê¸°ë°˜ ì‚¬ìš©ìë³„ ê°œë³„ ì œí•œ
- ğŸ”„ **OpenAI í˜¸í™˜ API**: í‘œì¤€ ChatGPT APIì™€ ì™„ì „ í˜¸í™˜
- ğŸ“Š **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì‚¬ìš©ëŸ‰ í†µê³„ ë° ëŒ€ì‹œë³´ë“œ
- ğŸš€ **ê³ ì„±ëŠ¥**: vLLM ê¸°ë°˜ GPU ê°€ì† ì¶”ë¡ 

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚â”€â”€â”€â–¶â”‚  Token Limiter   â”‚â”€â”€â”€â–¶â”‚   vLLM Server   â”‚
â”‚                 â”‚    â”‚   (Port 8080)    â”‚    â”‚   (Port 8000)   â”‚
â”‚ - Web App       â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ - Mobile App    â”‚    â”‚ - Rate Limiting  â”‚    â”‚ - GPU Inference â”‚
â”‚ - API Client    â”‚    â”‚ - User Managementâ”‚    â”‚ - Model Serving â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - Token Counting â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ - Statistics     â”‚              â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                       â”‚   Redis/SQLite   â”‚              â”‚
                       â”‚                  â”‚              â”‚
                       â”‚ - Usage Data     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ - User Stats     â”‚    â”‚ Korean LLM Modelâ”‚
                       â”‚ - Rate Limits    â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ - distilgpt2    â”‚
                                              â”‚ - beomi/llama   â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

- **Python**: 3.11 ì´ìƒ
- **GPU**: NVIDIA GPU (RTX 4060 ê¶Œì¥) + CUDA 12.1+
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **ì €ì¥ê³µê°„**: 10GB ì´ìƒ

### 1. ì €ì¥ì†Œ í´ë¡ 

```bash
git clone https://github.com/your-username/llama-korean-token-limiter.git
cd llama-korean-token-limiter
```

### 2. í™˜ê²½ ì„¤ì •

#### Conda í™˜ê²½ (ê¶Œì¥)

```bash
# Conda í™˜ê²½ ìƒì„±
conda create -n korean_llm python=3.11
conda activate korean_llm

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash scripts/install_conda_packages.sh
```

#### Python venv í™˜ê²½

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ë˜ëŠ”
venv\Scripts\activate  # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash scripts/install_packages.sh
```

### 3. Redis ì„¤ì •

#### Docker ì‚¬ìš© (ê¶Œì¥)

```bash
docker run -d --name korean-redis -p 6379:6379 redis:alpine
```

#### ë¡œì»¬ Redis ì„¤ì¹˜

```bash
# Ubuntu/Debian
sudo apt install redis-server

# macOS
brew install redis
```

### 4. ì‹œìŠ¤í…œ ì‹œì‘

```bash
# ì „ì²´ ì‹œìŠ¤í…œ ì‹œì‘ (vLLM + Token Limiter)
bash scripts/start_korean_system.sh
```

### 5. í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ì²´í¬
curl http://localhost:8080/health

# ì±„íŒ… ì™„ì„± í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-llama",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}],
    "max_tokens": 50
  }'
```

## ğŸ“š API ì‚¬ìš©ë²•

### ì¸ì¦

ëª¨ë“  API ìš”ì²­ì—ëŠ” Authorization í—¤ë”ê°€ í•„ìš”í•©ë‹ˆë‹¤:

```bash
Authorization: Bearer <API_KEY>
```

### ê¸°ë³¸ ì‚¬ìš©ì API í‚¤

| ì‚¬ìš©ì | API í‚¤ | ì œí•œ (RPM/TPM/ì¼ì¼) |
|--------|--------|-------------------|
| ì‚¬ìš©ì1 | `sk-user1-korean-key-def` | 20/3000/500K |
| ê°œë°œì1 | `sk-dev1-korean-key-789` | 50/8000/1.5M |
| í…ŒìŠ¤íŠ¸ | `sk-test-korean-key-stu` | 10/1000/100K |

### ì±„íŒ… ì™„ì„± API

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-llama",
    "messages": [
      {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
      {"role": "user", "content": "íŒŒì´ì¬ìœ¼ë¡œ Hello Worldë¥¼ ì¶œë ¥í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

### í…ìŠ¤íŠ¸ ì™„ì„± API

```bash
curl -X POST http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-user1-korean-key-def" \
  -d '{
    "model": "korean-llama",
    "prompt": "í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
    "max_tokens": 50,
    "temperature": 0.5
  }'
```

### í† í° ê³„ì‚°

```bash
curl 'http://localhost:8080/token-info?text=ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì–´ í† í° ê³„ì‚° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.'
```

### ì‚¬ìš©ëŸ‰ í†µê³„

```bash
# ì‚¬ìš©ìë³„ í†µê³„
curl http://localhost:8080/stats/user1

# ì „ì²´ ì‚¬ìš©ì ëª©ë¡
curl http://localhost:8080/admin/users
```

## âš™ï¸ ì„¤ì •

### ëª¨ë¸ ì„¤ì • (`config/korean_model.yaml`)

```yaml
server:
  host: "0.0.0.0"
  port: 8080

llm_server:
  url: "http://localhost:8000"
  model_name: "distilgpt2"  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸
  
  vllm_args:
    gpu_memory_utilization: 0.8
    max_model_len: 2048
    dtype: "half"
    enforce_eager: true

storage:
  type: "redis"  # ë˜ëŠ” "sqlite"
  redis_url: "redis://localhost:6379"

default_limits:
  rpm: 30      # ë¶„ë‹¹ ìš”ì²­ ìˆ˜
  tpm: 5000    # ë¶„ë‹¹ í† í° ìˆ˜
  tph: 300000  # ì‹œê°„ë‹¹ í† í° ìˆ˜
  daily: 500000 # ì¼ì¼ í† í° ìˆ˜
```

### ì‚¬ìš©ì ì„¤ì • (`config/korean_users.yaml`)

```yaml
users:
  ì‚¬ìš©ì1:
    rpm: 20
    tpm: 3000
    daily: 500000
    description: "ì¼ë°˜ ì‚¬ìš©ì"
    
  ê°œë°œì1:
    rpm: 50
    tpm: 8000
    daily: 1500000
    description: "ê°œë°œì ê³„ì •"

api_keys:
  "sk-user1-korean-key-def": "ì‚¬ìš©ì1"
  "sk-dev1-korean-key-789": "ê°œë°œì1"
```

## ğŸ–¥ï¸ ëŒ€ì‹œë³´ë“œ

Streamlit ê¸°ë°˜ ì›¹ ëŒ€ì‹œë³´ë“œë¡œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§:

<img src="demo.png" width="500" height="300">

```bash
# ëŒ€ì‹œë³´ë“œ ì‹œì‘
streamlit run dashboard/app.py --server.port 8501

# ì ‘ì†: http://localhost:8501
```

ëŒ€ì‹œë³´ë“œ ê¸°ëŠ¥:
- ğŸ“ˆ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ ê·¸ë˜í”„
- ğŸ‘¥ ì‚¬ìš©ìë³„ í†µê³„
- ğŸš¨ ì†ë„ ì œí•œ ì•Œë¦¼
- ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. vLLM ì„œë²„ ì‹œì‘ ì‹¤íŒ¨

```bash
# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
python -m vllm.entrypoints.openai.api_server \
  --model distilgpt2 \
  --gpu-memory-utilization 0.4 \
  --max-model-len 256
```

#### 2. íŒ¨í‚¤ì§€ ì¶©ëŒ

```bash
# vLLM í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
bash scripts/fix_vllm_compatibility.sh
```

#### 3. Redis ì—°ê²° ì‹¤íŒ¨

```bash
# SQLite ëª¨ë“œë¡œ ì „í™˜
sed -i 's/type: "redis"/type: "sqlite"/' config/korean_model.yaml
```

#### 4. í•œêµ­ì–´ ì¸ì½”ë”© ë¬¸ì œ

ì‹œìŠ¤í…œì—ì„œ ìë™ìœ¼ë¡œ ASCII ì•ˆì „ ì¸ì½”ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í•œêµ­ì–´ ì‚¬ìš©ìëª…ì€ ë‚´ë¶€ì ìœ¼ë¡œ ì˜ì–´ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.

### ë¡œê·¸ í™•ì¸

```bash
# Token Limiter ë¡œê·¸
tail -f logs/token_limiter.log

# vLLM ì„œë²„ ë¡œê·¸  
tail -f logs/vllm.log

# ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ
curl http://localhost:8080/health
```

### ì„±ëŠ¥ ìµœì í™”

#### RTX 4060 8GB ìµœì í™”

```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  40%ë¡œ ì œí•œ
--gpu-memory-utilization 0.4

# ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ë‹¨ì¶•
--max-model-len 256

# FP16 ì‚¬ìš©
--dtype half
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

```bash
bash scripts/test_korean.sh
```

### ê°œë³„ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸

```bash
# vLLM ì§„ë‹¨
python test_vllm_simple.py

# í† í° ì¹´ìš´í„° í…ŒìŠ¤íŠ¸
curl 'http://localhost:8080/token-info?text=í…ŒìŠ¤íŠ¸'

# ì†ë„ ì œí•œ í…ŒìŠ¤íŠ¸
for i in {1..10}; do
  curl -X POST http://localhost:8080/v1/chat/completions \
    -H "Authorization: Bearer sk-test-korean-key-stu" \
    -d '{"model":"korean-llama","messages":[{"role":"user","content":"Test '$i'"}],"max_tokens":10}'
done
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### RTX 4060 Laptop GPU ê¸°ì¤€

| ëª¨ë¸ | í† í°/ì´ˆ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë™ì‹œ ì‚¬ìš©ì |
|------|---------|--------------|------------|
| distilgpt2 | ~150 | 2.5GB | 4-6ëª… |
| gpt2 | ~120 | 3.2GB | 3-4ëª… |
| beomi/llama-2-ko-7b | ~45 | 7.5GB | 1-2ëª… |

### í† í° ê³„ì‚° ì„±ëŠ¥

- í•œêµ­ì–´ í…ìŠ¤íŠ¸: ~5000 ê¸€ì/ì´ˆ
- ì˜ì–´ í…ìŠ¤íŠ¸: ~8000 ê¸€ì/ì´ˆ
- í˜¼í•© í…ìŠ¤íŠ¸: ~6000 ê¸€ì/ì´ˆ

## ğŸ”’ ë³´ì•ˆ

### API í‚¤ ê´€ë¦¬

- API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë‚˜ ì•ˆì „í•œ ì„¤ì • íŒŒì¼ì— ì €ì¥
- í”„ë¡œë•ì…˜ì—ì„œëŠ” JWT í† í°ì´ë‚˜ OAuth ì‚¬ìš© ê¶Œì¥
- HTTPS ì‚¬ìš© í•„ìˆ˜

### ì†ë„ ì œí•œ

- ê¸°ë³¸ì ìœ¼ë¡œ IPë³„ ì œí•œì€ ë¹„í™œì„±í™”
- í•„ìš”ì‹œ `rate_limit_by_ip: true` ì„¤ì •
- DDoS ë°©ì–´ë¥¼ ìœ„í•œ ì›¹ ì„œë²„(Nginx) ì‚¬ìš© ê¶Œì¥

## ğŸš¢ ë°°í¬

### Docker ë°°í¬

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t korean-token-limiter .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  --name korean-limiter \
  --gpus all \
  -p 8080:8080 \
  -v $(pwd)/config:/app/config \
  korean-token-limiter
```

### í”„ë¡œë•ì…˜ ë°°í¬

```bash
# Gunicorn ì‚¬ìš©
gunicorn main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8080 \
  --access-logfile logs/access.log
```

